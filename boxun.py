# -*- coding:utf-8 -*-
import requests
import traceback
import re
import os
import itchat
import time
from retry import retry
from termcolor import colored
from bs4 import BeautifulSoup

# 工具类函数
def log_info(msg):
    print(colored('[ * ] %s' % (msg), 'blue'))
    
def log_success(msg):
    print(colored('[ + ] %s' % (msg), 'green'))

def log_warning(msg):
    print(colored('[ - ] %s' % (msg), 'yellow'))

def log_error(msg):
    print(colored('[ ! ] %s' % (msg), 'red'))

# 保存位置
base_dir = os.path.dirname(__file__)
results_dir = os.path.join(base_dir,'results')
results_dir='/var/www/html/results'
if not os.path.exists(results_dir):
    os.mkdir(results_dir)

# 爬取配置
interval = 5
main_host = 'https://boxun.com'
main_url = main_host + '/news/gb/china/page1.shtml'
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'
}
# 本地shadowsocks代理, http代理端口
proxy = '127.0.0.1:8118'
proxies = {
    'http': 'http://' + proxy,
    'https': 'https://' + proxy
}
timeout = 10

itchat.auto_login(enableCmdQR=True)
users=itchat.search_friends("W")
userName= users[0]['UserName']
print(userName)

@retry(requests.exceptions.RequestException, tries=3, delay=2)
def retry_get(url):
    global proxies
    if not proxy:
        proxies = {}
    return requests.get(url, headers=headers ,proxies=proxies ,timeout=timeout)

def cget(url):
    try:
        return retry_get(url)
    except requests.exceptions.RequestException:
        log_warning('error occourd when request url {}'.format(url))

def save_url(url, title):
    artical_url = main_host + url
    artical_resp = cget(artical_url)
    artical_content = artical_resp.content.decode('gb2312','ignore')
    soup = BeautifulSoup(artical_content, 'lxml')
    base_name = os.path.basename(artical_url)
    base_name = base_name.split('.')[0] + '.htm'
    with open(os.path.join(results_dir,base_name), 'w') as f:
        f.write(str(soup))
    log_success('request success with url {}'.format(url))

def run():
    main_resp = cget(main_url)
    main_content = main_resp.content.decode('gb2312','ignore')
    soup = BeautifulSoup(main_content, 'lxml')
    day_title_list = soup.find_all(name='td',text=re.compile('\d{4}年\d{2}月\d{2}日'))
    day_list = []
    for day_title in day_title_list:
        day_list.append(day_title.parent.parent)
    # titile中存在图的话，bs4无法正确识别，所以自定义
    re_title = re.compile('target="_blank">(.*?)<')
    for day_content in day_list:
        url_list = day_content.find_all(name='a',attrs={'target':'_blank'})
        for url in url_list:
            href = url.attrs['href']
            title = re_title.findall(str(url))[0].strip()
            base_name = os.path.basename(href)
            base_name = base_name.split('.')[0] + '.htm'
            # title 标题，base_name 文件保存为的名字
            if os.path.exists(os.path.join(results_dir,base_name)):
                log_info('url {} saved, skip'.format(href))
            else:
                sendData = title+" http://111.230.63.130/results/"+base_name
                itchat.send(sendData,toUserName=userName)
                save_url(href,title)

if __name__ == "__main__":
    try:
        while True:
            try:
                log_info('start new try when {}'.format(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))))
                run()
            except Exception:
                traceback.print_exc()
                log_error('unexpected error occourd')
            finally:
                time.sleep(interval)
    except KeyboardInterrupt:
        log_error('User Exit, Bye :)')